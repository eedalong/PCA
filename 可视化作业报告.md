## 可视化作业报告

### 一、MNIST数据集 PCA可视化

### 1、PCA原理

PCA基于矩阵SVD分解进行计算。我们首先回顾下特征值和特征向量的定义如下：
$$
Ax = \lambda x
$$
其中当A是一个nxn的实对称矩阵，x是一个𝑛维向量，则我们说λ是矩阵A的一个特征值，而𝑥是矩阵A的特征值𝜆所对应的特征向量。求出特征值和特征向量我们可以将矩阵A特征分解。如果我们求出了矩阵A的𝑛个特征值𝜆1≤𝜆2≤...≤𝜆𝑛,以及这𝑛个特征值所对应的特征向量{𝑤1,𝑤2,...𝑤𝑛}，，如果这𝑛个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：
$$
A = W\Sigma W^{-1}
$$
其中W的每一列为A的特征向量，Sigma为对角阵，对角阵上的每一个值为特征向量对应的特征值，同时W中的特征向量彼此正交。实际上W中的特征向量作为新的空间的基底，对于每一个输入向量 x， Ax实际上得到的是x在A的空间基底上的投影的分量。而特征值较大的基底意味着绝大部分向量在此方向的投影分量较大，故而占比的信息最多。我们以信号的分解和压缩举例子，那么特征值的大小实际上表示着信号在该维度上的分量的能量强度，为了压缩信号，我们可以省去末尾能量较小即特征值较小的特征向量来较好的恢复原始信号。那么我们取前K个分量作为空间坐标，将每一个输入向量投影到该降维的空间中，我们得到了一个在该矩阵空间中丢失信息最少的K维表示，而这也是我们进行数据降维的基础。

### 2、MNIST数据可视化

首先我们直接对原始数据进行PCA降维到2维并进行可视化，我们发现总体而言还是可以看出相同的数据有聚集到一起的趋势。但是分开的不明显。分析可以得到原始数据的特征本身不够明显，对图片特征的刻画不够本质，所以直接做PCA效果并不会很好。

![raw_feature](/Users/bytedance/CodeBases/PCA/raw_feature.jpg)

接着我们搭建深度学习模型MNIST Model并使用数据进行训练，经过两个epoch的训练，模型在MNIST数据集上达到了98.73%的准确率。我们使用该模型的最后一个FC层的输出作为图像的特征进行输出作为该图像的特征，并基于该特征进行PCA的分解和可视化。我们发现使用MNIST模型处理后的图像特征作为特征进行PCA后，数据的分离更加明显了。

![mnist_feature](/Users/bytedance/CodeBases/PCA/mnist_feature.jpg)

